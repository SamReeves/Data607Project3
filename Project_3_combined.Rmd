---
title: "Project 3"
authors: "Sam Reeves, Henry Owens, Claire Meyer, Michael Ippolito"
date: "3/23/2021"
output:
  html_document:
    df_print: paged
    toc: false
    toc_float: true
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE}
library(RCurl)
library(httr)
library(jsonlite)
library(tidyverse)
library(tokenizers)
library(stopwords)
library(kableExtra)
library(RMySQL)
library(wordcloud)
library(RColorBrewer)
library(tm)
library(ggplot2)
```
# Most Valuable Data Science Skills

This document walks through scraping website search results for information on top data science skills, scraping the CUNY website for course descriptions, and comparing the content of each. These results are pushed to a MySQL database. 


## Getting the Search data
Below we run a function to hit the Custom Search API configured to search monster.com, indeed.com, and towardsdatascience.com. These results are collected, tidied and turned into a data frame of 100 results. 100 is the maximum number of results in the free version of the API. 

```{r dataset}

# Download the file so that we don't have to scrape the website every time we want to run this chunk.
download.file("https://raw.githubusercontent.com/TheWerefriend/Data607Project3/main/search_results.csv",
              destfile="search_results.csv", method="libcurl")

if (file.exists("search_results.csv")) {
  
  base_df = read.csv("search_results.csv")

} else {

  # We can start with the first 10 results
    base_url = "https://www.googleapis.com/customsearch/v1?key=AIzaSyDGGlvHG0qLpOjLF5_k-ojZtPKjyFOSUYM&cx=1f1bbcd0558947d90&q='data+scientist+skills'"
    base_results <- fromJSON(txt=base_url)
    
  # Create data frame, discarding the 11th column, which we don't need and is problematic
  # because it's a nested list.
  base_df <- base_results$items[, -11]
    
  # Create a query base to iterate on
  query_base <- "https://www.googleapis.com/customsearch/v1?key=AIzaSyDGGlvHG0qLpOjLF5_k-ojZtPKjyFOSUYM&cx=1f1bbcd0558947d90&exactTerms='data+science'&q='data+scientist+skills'&start=" 
    
  # Created a search results function that inputs query string 
  # and outputs results that can be bound to base_df
  get_search_results <- function(q_string) {
      results <- fromJSON(txt = q_string)
      items_df <- results$items[, -11]
      return(items_df)
  }
    
  # Run a for loop to generate the needed queries for the next 90 results. 
  # We append a start index to the query base to grab the next set of results.
  x <- c(1:9)
  for (i in x) {
    query <- paste(query_base, (i * 10 + 1), sep="")
    items_df <- get_search_results(query)
    base_df <- rbind(base_df, items_df)
  }
  
  # In case of rate limiting, write to CSV
  write.csv(base_df,'search_results.csv',row.names = TRUE)

}

```

## Create a stopword list

This list represents the generic words we don't want to include when we ultimately compare and rank the results, like 'the' or 'and'.

```{r stopwords}
# Tokenize snippets
# https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html
# Fetch stopword list
stopwordlist <- stopwords::stopwords("en")
# Remove bare numbers
stopwordlist <- append(stopwordlist, sapply(c(1:100), "as.character"))
# Remove months and years
stopwordlist <- append(stopwordlist, c("2020", "2021", "2019", "2018", "2017", tolower(month.abb[c(1:12)])))
# Remove other miscellaneous words
stopwordlist <- append(stopwordlist, c("15,000", "11000", "also", "ago"))
```

## Create Search Results corpus

We combine result snippets into a corpus of text, remove non-alphabetical characters and tokenize. We then have those tokens as a list.

```{r corpus}
# Create lowercase corpus of skills
skills_corpus_lower <- tolower(paste(base_df$snippet, collapse = ""))
# Remove stop words from the corpus
for (stopword in stopwordlist) {
  replaceRegex = paste("\\b", stopword, "\\b", sep = "")
  skills_corpus_lower <- str_replace_all(skills_corpus_lower, replaceRegex, "")
}
# Remove non alphabetical characters (except apostrophes) from the corpus,
# then remove any duplicate spaces it may have created as a result
skills_corpus_lower <- str_replace_all(skills_corpus_lower, "[^a-z']", " ")
skills_corpus_lower <- str_replace_all(skills_corpus_lower, " +", " ")
# Tokenize using n-grams between 1 and 3 words long
skills_tokens <- tokenize_ngrams(skills_corpus_lower, n = 3, n_min = 1, stopwords = stopwordlist)[[1]]
skills_tokens <- as.list(skills_tokens)
```

## CUNY scrape

Then we can scrape the CUNY Master's in Data Science course catalog, identifying course descriptions.

```{r cuny_scrape}

# Download the file so that we don't have to scrape the website every time we want to run this chunk.
download.file("https://raw.githubusercontent.com/TheWerefriend/Data607Project3/main/cuny.csv",
              destfile="cuny.csv", method="libcurl")

if (file.exists("cuny.csv")) {
  
  dfcuny <- read.csv("cuny.csv")

} else {

  # Read CUNY master's in data science course catalog
  cunyUrl <- "http://catalog.sps.cuny.edu/preview_program.php?catoid=2&poid=607"
  catalog <- getURL(cunyUrl)
  
  # Each course will show up like this in the html source:
  # <a href="#" aria-expanded="false" onClick="showCourse('2', '979',this, 'a:2:{s:8:~location~;s:7:~program~;s:4:~core~;s:4:~1598~;}'); return false;">DATA 602 - Advanced Programming Techniques (3 Credits) </a>
  # Use regex to parse
  
  matches <- str_match_all(catalog, "showCourse\\('(\\d+)', *'(\\d+)',this, '(.+?)'.+?>(DATA \\d{3}) - (.+?) \\(?(\\d+.*?) Credits\\)?.+?<\\/a>")
    
  # Build URL to fetch each course
  # We need to end up with this format:
  # http://catalog.sps.cuny.edu/ajax/preview_course.php?catoid=2&coid=979&link_text=&display_options=a:2:{s:8:~location~;s:7:~program~;s:4:~core~;s:4:~1598~;}&show
  courseUrls <- str_c("http://catalog.sps.cuny.edu/ajax/preview_course.php?catoid=",
          matches[[1]][,2], "&coid=", matches[[1]][,3], "&link_text=&display_options=", matches[[1]][,4], "&show")
    
  # Build dataframe
  dfcuny <- data.frame(courseno = matches[[1]][,5], descr = matches[[1]][,6], credits = matches[[1]][,7], url = courseUrls)
  
  # Fetch course descriptions using courseUrls
  results <- sapply(dfcuny$url, function(x) try(getURL(x)))
  
  # Extract first element from results
  tmpa <- sapply(results, "[", 1)
  
  # Parse results; this is the part we're interested in:
  # DATA 607</a><span style="display: none !important">&#160;</span></em><br>In this course students will learn...<br><br>
  coursematches <- str_match_all(tmpa, regex('<\\/em>.*<br>(.+?)<br><br>', dotall = TRUE))
  
  # Extract second element from results
  tmpb <- sapply(coursematches, "[", 2)
  
  # Add the course text to the data frame
  dfcuny <- mutate(dfcuny, course_text = tmpb)
  
  # Write to CSV
  write.csv(dfcuny,'cuny.csv',row.names = TRUE)
}
```
  
## CUNY corpus

With the same pattern as search results, we combine course text into a corpus, remove the same stop words, and tokenize in the same manner.

```{r cuny_corpus}
# Create cuny corpus
cuny_corpus_lower <- tolower(paste(dfcuny$course_text, collapse = ""))

# Remove stop words from the corpus
for (stopword in stopwordlist) {
  replaceRegex = paste("\\b", stopword, "\\b", sep = "")
  cuny_corpus_lower <- str_replace_all(cuny_corpus_lower, replaceRegex, "")
}

# Remove non alphabetical characters (except apostrophes) from the corpus,
# then remove any duplicate spaces it may have created as a result
cuny_corpus_lower <- str_replace_all(cuny_corpus_lower, "[^a-z']", " ")
cuny_corpus_lower <- str_replace_all(cuny_corpus_lower, " +", " ")

# Tokenize using n-grams between 1 and 3 words long
cuny_tokens <- tokenize_ngrams(cuny_corpus_lower, n = 3, n_min = 1, stopwords = stopwordlist)[[1]]
cuny_tokens <- as.list(cuny_tokens)
```

## Count Term Frequency

Then we can count the number of occurrences of each token in the skills corpus and in the CUNY course catalog corpus.

```{r counts}
# Combine the skills and cuny token lists and remove duplicates
all_tokens <- c(skills_tokens, cuny_tokens)
all_tokens <- unique(all_tokens)

# Create data frame to hold counts
count_table <- data.frame(c(1:length(all_tokens)), 0, 0)
colnames(count_table) <- c("token", "skills_count", "cuny_count")

# Walk each token and count the number of occurrences in each corpus
for (i in c(1:length(all_tokens))) {
  
  # Add an entry in the counts table for this token
  count_table$token[i] <- as.character(all_tokens[i])
  
  # Build regex based on this token occurring along word boundaries
  tokenRegex <- paste("\\b", as.character(all_tokens[i]), "\\b", sep = "")
  
  # Count the number of occurrences of this token in the skills corpus
  count_table$skills_count[i] <- str_count(skills_corpus_lower, pattern = tokenRegex)
  
  # Count the number of occurrences of this token in the cuny corpus
  count_table$cuny_count[i] <- str_count(cuny_corpus_lower, pattern = tokenRegex)
}

# Add a % of snippets field
count_table <- count_table %>% 
  mutate(pct_skills_snippets = skills_count / 100) %>%
  mutate(pct_cuny_courses = cuny_count / 19)

# Write to CSV
write.csv(count_table, 'counts.csv', row.names = TRUE)

```


## Populating MySQL tables

With the data collected, we can push this data to the MySQL db we've developed to host our results tables. 

First we prep the final dataframes that will make up our tables.

```{r prep_dfs}
df_search <- base_df %>%
  select(link,snippet)

df_compare <- count_table %>%
  select(token,cuny_count,skills_count)

cuny_count <- count_table %>%
  select(token,cuny_count)

search_count <- count_table %>%
  select(token,skills_count)
```

Then we setup the db.

```{r init_db}

# Connect to db
drv = dbDriver("MySQL")
con = dbConnect(drv,
                host = "134.122.18.100",
                port = 3306,
                user = "normie",
                password = "doge",
                dbname = "skills")

# Set up tables for each of our results tables.
# Use tryCatch block in case the tables are already created

# cuny results table
cuny_df_query <- "CREATE TABLE df_cuny(
  courseno VARCHAR(7),
  descr VARCHAR(255),
  credits INT(11),
  url VARCHAR(255),
  course_text VARCHAR(255)
  );"
tryCatch(
  df_cuny_create <- dbSendQuery(con, cuny_df_query),
  warning = function (w) {print(paste("Warning: ", w))},
  error = function (e) {print(paste("Error: ", e))}
)

# search results tble
search_df_query <- "CREATE TABLE df_search(
  link VARCHAR(255),
  snippet VARCHAR(255)
  );"
tryCatch(
  df_search_create <- dbSendQuery(con, search_df_query),
  warning = function (w) {print(paste("Warning: ", w))},
  error = function (e) {print(paste("Error: ", e))}
)

# compare table
compare_df_query <- "CREATE TABLE df_compare(
  token VARCHAR(255),
  cuny_count INT(11),
  skills_count INT(11)
  );"
tryCatch(
  df_compare_create <- dbSendQuery(con, compare_df_query),
  warning = function (w) {print(paste("Warning: ", w))},
  error = function (e) {print(paste("Error: ", e))}
)

# cuny count table
cuny_count_query <- "CREATE TABLE cuny_count(
  token VARCHAR(255),
  cuny_count INT(11)
  );"
tryCatch(
  cuny_count_create <- dbSendQuery(con, cuny_count_query),
  warning = function (w) {print(paste("Warning: ", w))},
  error = function (e) {print(paste("Error: ", e))}
)

# search results count table
search_count_query <- "CREATE TABLE search_count(
  token VARCHAR(255),
  skills_count INT(11)
  );"
tryCatch(
  df_compare_create <- dbSendQuery(con, search_count_query),
  warning = function (w) {print(paste("Warning: ", w))},
  error = function (e) {print(paste("Error: ", e))}
)

# List the tables
dbListTables(con)

# Now we can populate the tables with the configured data frames. 
# First see if the tables have already been
# populated. If not, set append to TRUE and overwrite to FALSE. If so,
# set append to FALSE and overwrite to TRUE.

# cuny results table
dbres <- dbSendQuery(con, "select count(*) as ct from df_cuny;")
dbdata <- fetch(dbres, n = -1)
if (dbdata$ct > 1) { dbappend = FALSE } else { dbappend = TRUE }
columns <- colnames(dfcuny)
dbWriteTable(con,
             value = dfcuny,
             row.names = FALSE,
             name = "df_cuny",
             overwrite = !dbappend,
             append = dbappend)

# search results table
dbres <- dbSendQuery(con, "select count(*) as ct from df_search;")
dbdata <- fetch(dbres, n = -1)
columns <- colnames(df_search)
dbWriteTable(con,
             value = df_search,
             row.names = FALSE,
             name = "df_search",
             overwrite = !dbappend,
             append = dbappend)

# compare table
dbres <- dbSendQuery(con, "select count(*) as ct from df_compare;")
dbdata <- fetch(dbres, n = -1)
columns <- colnames(df_compare)
dbWriteTable(con,
             value = df_compare,
             row.names = FALSE,
             name = "df_compare",
             overwrite = !dbappend,
             append = dbappend)

# cuny count table
dbres <- dbSendQuery(con, "select count(*) as ct from cuny_count;")
dbdata <- fetch(dbres, n = -1)
columns <- colnames(cuny_count)
dbWriteTable(con,
             value = cuny_count,
             row.names = FALSE,
             name = "cuny_count",
             overwrite = !dbappend,
             append = dbappend)

# search results count table
dbres <- dbSendQuery(con, "select count(*) as ct from search_count;")
dbdata <- fetch(dbres, n = -1)
columns <- colnames(search_count)
dbWriteTable(con,
             value = search_count,
             row.names = FALSE,
             name = "search_count",
             overwrite = !dbappend,
             append = dbappend)

# From here we can close our connection.
dbDisconnect(con)

```

## Analysis

Our model produced a large amount of data-science-relevant words and terms from job postings and from that we are able to discern what are some of the most important skills in the market for data scientists. Our first chart compares the text from the skills corpus to the text from the CUNY MSDS course descriptions, counting the matches. 

Most important finding: DATA 607 finishes in a strong third place!


```{r warning=FALSE}
# Add column to dfcuny counting up the instances of skills tokens
dfcuny$tally_tokens <- str_count(dfcuny$course_text, paste0(count_table$token,
                                                                     collapse = '|'))
# remove junk text
dfcuny$descr <- str_replace_all(dfcuny$descr, "&.*;", "")
# Plot the results
ggplot2::ggplot(dfcuny, aes(reorder(descr, tally_tokens, sum), tally_tokens)) +
  geom_col() +
  ylab("DS Skills in Course Description") +
  xlab("") +
  ggtitle("Courses with the most Skills Tokens") +
  coord_flip()

```
## Wordclouds

Next we construct some word clouds to get a sense of which terms are most prominent in both corpuses. The skills corpus at first returns a word cloud heavily favoring variations on our search terms: "data science skills". 

```{r skills-wordcloud, warning=FALSE}
# configure corpus
corpus_cloud_sk <- Corpus(VectorSource(skills_corpus_lower))

# create term document matrix and df for skills wordcloud
dtm1 <- TermDocumentMatrix(corpus_cloud_sk) 
matrix1 <- as.matrix(dtm1) 
words1 <- sort(rowSums(matrix1),decreasing=TRUE) 
df_cloud_sk <- data.frame(word = names(words1),freq=words1) 

# generate wordcloud for skills
set.seed(1234) # for reproducibility 
wordcloud(words = df_cloud_sk$word, freq = df_cloud_sk$freq, min.freq = 1,
          max.words =200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))
```
We remove the first few rows of the words closely matching our search terms to get a more informative word cloud highlighting such skills as: Python, programming, SQL, machine learning, and also some softer skills such as communication.

```{r warning=FALSE}

df_cloud_sk2 <- df_cloud_sk[7:nrow(df_cloud_sk),]

# generate wordcloud for skills
set.seed(1234) # for reproducibility 
wordcloud(words = df_cloud_sk2$word, freq = df_cloud_sk2$freq, min.freq = 1,           
          max.words=200, random.order=FALSE, rot.per=0.35, scale = c(2.5, 0.25),            
          colors=brewer.pal(8, "Dark2"))
```

The CUNY wordcloud reveals a slightly more even distribution of words from the initial text.  

```{r cuny-wordcloud, message=FALSE, warning=FALSE}
# configure corpus
corpus_cloud_cuny <- Corpus(VectorSource(cuny_corpus_lower))

# create term document matrix and df for cuny wordcloud
dtm2 <- TermDocumentMatrix(corpus_cloud_cuny) 
matrix2 <- as.matrix(dtm2) 
words2 <- sort(rowSums(matrix2),decreasing=TRUE) 
df_cloud_cuny <- data.frame(word = names(words2),freq=words2)

# generate wordcloud for cuny
set.seed(1234) # for reproducibility 
wordcloud(words = df_cloud_cuny$word, freq = df_cloud_cuny$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))
```

